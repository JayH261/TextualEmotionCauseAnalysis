{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This file is to preprocess data to json file\n",
    "Jay Ho, David Khankin\n",
    "'''\n",
    "import json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data path\n",
    "train_data_path = \"text_train/Subtask_1_2_train.json\"\n",
    "# Load json training data\n",
    "with open(train_data_path, \"r\") as read_file:\n",
    "    train_dataset = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # convert all lowercase\n",
    "    new_str = text.lower()\n",
    "    # convert all occurrences of the following markup strings in the tweet message:\n",
    "    # \\&quot; \\&amp; \\&gt; and \\&lt into white space characters\n",
    "    new_str = re.sub('&quot;|&amp;|&gt;|&lt;', \" \", new_str)\n",
    "    #replace every occurrence of the following special characters in the tweet message\n",
    "    new_str = re.sub(r'[\\t,;\\\"!\\\"\\?\\\"+\\=\\*\\|\\(\\)\\[\\]\\{}]', \" \", new_str)\n",
    "    new_str = re.sub(re.compile('(?:\\.){2,}'),\" \",new_str)\n",
    "    new_str = re.sub(re.compile('(?:\\.\\s|^\\s+|\\s+$)'), \" \", new_str)\n",
    "    new_str = re.sub(r'^\\.+|\\.+$', ' ', new_str)\n",
    "    new_str = re.sub(re.compile('(?:\\s){2,}'),\" \",new_str)\n",
    "    new_str = re.sub(re.compile('<.*?>'), '', new_str)\n",
    "    new_str = new_str.strip() # remove any extra whitespace at the end of the line\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_list = []\n",
    "\n",
    "# visit each conversation\n",
    "for i in range(len(train_dataset)):\n",
    "    conversation_dict = dict()\n",
    "    conversation_id = int(train_dataset[i][\"conversation_ID\"])\n",
    "    conversation_dict[\"conversation_id\"] = conversation_id\n",
    "    # visit each utterance within a conversation and save to the csv file\n",
    "    # add emo-cause pairs list\n",
    "    emo_cause_pairs = []\n",
    "    # for loop emo cause pairs\n",
    "    for pair in train_dataset[i][\"emotion-cause_pairs\"]:\n",
    "        emotion_text = pair[0].split(\"_\")\n",
    "        cause_text = pair[1].split(\"_\")\n",
    "        # pair is (emotion utterance id - integer, cause utterance id - integer)\n",
    "        emo_cause_pairs.append((int(emotion_text[0]), int(cause_text[0])))\n",
    "        \n",
    "    conversation_dict[\"emo_cause_pairs\"] = emo_cause_pairs\n",
    "    \n",
    "    # add to conversation dictionary\n",
    "    utterance_list = []\n",
    "    for utterance in train_dataset[i][\"conversation\"]:\n",
    "        # {\"utterance_id\": integer, \"emotion\": string, \"speaker\": string, \"utterance_text\": string}\n",
    "        utterance_dict = dict()\n",
    "        utterance_dict[\"utterance_id\"] = int(utterance[\"utterance_ID\"])\n",
    "        utterance_dict[\"emotion\"] = utterance[\"emotion\"]\n",
    "        utterance_dict[\"speaker\"] = utterance[\"speaker\"]\n",
    "        utterance_dict[\"utterance_text\"] = preprocess(utterance[\"text\"])\n",
    "        utterance_list.append(utterance_dict)\n",
    "    conversation_dict[\"utterances\"] = utterance_list\n",
    "    conversation_list.append(conversation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emocause_data.json\", \"w\") as outfile:\n",
    "    json.dump(conversation_list, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jayenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
